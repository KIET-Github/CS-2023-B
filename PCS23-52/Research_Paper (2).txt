Applications of Machine Learning:

Abstract :
Large amounts of data are widely available today. In order to extract some relevant information from this data and to create an algorithm based on this analysis, it is crucial to examine the data.
This is where data mining and machine learning come in. Machine learning is a subset of artificial intelligence that is used to build algorithms based on historical correlations and data trends. Machine learning is used in a wide range of fields, such as bioinformatics, intrusion detection, information retrieval, gaming, marketing, virus detection, image deconvolution, and others. This publication presents the work produced by a number of authors in the field of machine learning across a variety of application domains.
Introduction :
Machine learning was created through the study of computational learning theory and pattern recognition. It is the most effective method for employing models and algorithms to make predictions in the field of data analytics.
These analytical models allow researchers, engineers, data scientists, and analysts to produce reliable results and conclusions. It also aids in the uncovering of some hidden patterns or traits through historical insights and data trends [1]. The most important aspect of machine learning is feature selection [5]. Since the model is constructed based on the results from the training data, machine learning approaches are not interactive. It examines earlier observations to create precise forecasts. A very difficult task is creating a precise prediction rule on which to base an algorithm [2].
Gathering examples of both sorts of emails will help if, for instance, machine learning is needed to distinguish between spammed and non-spammed emails. The machine-learning algorithm uses these examples to construct an accurate prediction rule to ascertain whether or not the emails are spam [2].
With an optimal number of observations and findings, ML can be used to solve situations when theoretical understanding is still lacking [16]. The literature review and conclusion are covered in Sections II and III, respectively.
Literature Review:
Miroslav Kubat et al. talked about using machine learning to detect oil spills from radar photos in 1998 [7]. Several machine learning-related issues were explored, along with solutions. Experimental studies addressing the two core issues with machine learning were also covered.
The issues are batching and uneven training sets. The two algorithms SHRINK and one-sided selection were also discussed. They discovered that SHRINK might be applied to lessen the frequency of false alarms.
Using support vector machines, Asa Ben-Hur et al. introduced a novel approach for clustering in 2001 [4]. Using a Gaussian Kernel, data points were mapped onto a high-dimensional space, and when they were returned to the data space, they were divided into several clusters.
An approach is described in order to locate these clusters. The following benefits of the suggested algorithm were addressed by the authors: The suggested approach generates cluster borders of any shape while avoiding pointless operations, resulting in great efficiency.
Robert E. Schapire reviewed the boosting research in 2003 [2], examining the generalisation and training error of AdaBoost, the relationship between boosting and logistic regression, the applicability of boosting to linear programming and game theory, and the incorporation of human knowledge in boosting. The author mentioned the following benefits of AdaBoost: AdaBoost is used to find outliers and reduce the error brought on by a few mistakes on the training set. Adaboost programming is simple, fast, and easy.
The suggested technique is impartial to the size and number of clusters, and the algorithm used is SVC, which is based on SVM. Two variables, p and q, were employed by the authors. The parameters q and p are used to select the data probing scale and the quantity of outliers, respectively. As these parameters are increased, clusters are more likely to break apart.
Jose M. Jerez et al. evaluated the efficacy of statistical imputation and machine learning algorithms to identify patient duplication in a breast cancer data set in 2010 [3]. These strategies' results were then contrasted with those of the list wise deletion imputation technique. K-nearest Neighbor, multi-layer perceptrons, self-organizing maps (SOM), multiple imputation, mean, and hot-deck are a few of the imputation techniques based on machine learning techniques.
The database included information on 3679 women from 32 different institutions who received a diagnosis of breast cancer. The results showed that machine learning imputation techniques outperformed statistical imputation techniques in terms of results.
For mapping and tracking changes in land cover in rural areas, J.R. Otukei and T. Blaschke examined support vector machines, decision trees, and classification in 2010 [6]. With regard to these three objectives, the investigation of prospective data mining methods for the determination of suitable bands for classification, the performance assessment of all three methods, and the detection of changes in land cover were all completed. Before the examination of the data, preprocessing was done using ENVI 4.5 and ERDAS IMAGINE 9.1.When applied to the data, decision trees outperformed and outperformed the other two approaches in terms of performance and accuracy. Also estimated was failure deterioration.
The combination of logistic regression and relevance vector machine was suggested by Wahyu Caesarendra et al. in 2010 [9] for evaluating the failure deterioration and predicting failures before they really happen. Measured failure deterioration
Utilizing vector analysis and logistic regression, the results were evaluated. Then, these vectors were trained using the relevance vector machine. The proposed method was evaluated using failure simulation data and run-to-failure data. For this, a one-dimensional characteristic called kurtosis is determined, and LR and RVM are used to predict each unit of a machine component. The success of the training was evaluated using correlation and root mean squared error.
Degang Chen et al. improved the hard margin SVMs in 2010 [10]. They did this by taking into account the membership of each tuple that is to be trained. The fuzzy rough set method was used to achieve this. It was first proposed to use the fuzzy transitive kernel, which is based on fuzzy rough sets. A lower approximation operator was used to determine the membership of each training input in binary classification.The proposed approach was then compared using fuzzy SVMs and soft margin SVMs after that. The trials demonstrated the validity, stability, and relationship between fuzzy theory and SVMs of the suggested strategy.
Dursun Delen developed models in 2010 [12] to investigate and predict the reasons behind first-year students' breakdown. The factors that might affect their retention were looked at in this regard. Five years' worth of institution-specific data were used to develop the models, along with certain data mining techniques. The effectiveness of the prediction models was calculated using the 10-fold cross validation method. Throughout this process, the entire dataset was divided into 10 distinct, mutually independent subgroups. These models showed which children would attend school through their second year and which ones would drop out. The SVM generated better results than logistic regression, decision trees, and neural networks.
Sajjad Ahmad et al. published SVM in 2010 [14], a regression technique, and employed it to determine the soil moisture using remote sensing data. This model was used at ten places to determine the soil moisture in the western United States. Five years of data, from 1998 to 2002, were used to train the SVM model, and three years of data, from 2003 to 2005, were used to evaluate it. To evaluate SVM performance, two models were built. Data from the first model, which was initially trained and then evaluated, was used to generate six independent models for six different locations.For the second model, which was tested on the remaining four sites, the data from all the sites used in model one were integrated into a single model. Results showed that SVM performed better than MLR and ANN models.
Fan Min et al. presented the feature selection with test constraint problem in 2012 [5] as a solution to the issue of test cost restrictions caused by a shortage or scarcity of resources. The four design elements that were used to create the selection of the feature with the test constraint were constraint, input, optimization, and output objective. For this problem, a backtracking technique was developed for small and medium-sized datasets, while a heuristic approach was developed for extremely large datasets.
On four datasets, the suggested algorithm's performance was evaluated. The backtracking algorithm has shown to be effective for medium-sized data sets, but heuristic algorithms are generally more effective and stable than backtracking algorithms.
Conclusion:
The many machine learning techniques and approaches in numerous disciplines and application areas have been covered. Data mining and machine learning are comparable, however the former technique develops new algorithms or models based on observations and analysis, whereas the later approach focuses only on analysis. We have discussed the various uses of machine learning, such as image deconvolution, student retention, oil spill detection, and changing land cover. This gave us a fundamental overview of machine learning and its potential applications.
References:
[1].Machine Learning: What it is and why it matters". www.sas.com. Retrieved 2016-09-25.

[2].Schapire, R. E. (2003). The boosting approach to machine learning: An overview. In Nonlinear estimation and classification. Springer New York, 149-171.
 [3].Jerez, J. M., Molina, I, Garcia-Laencina, P. J., Alba, E., Ribelles, N., Martin, M., & Franco, L. (2010). Missing data imputation using statistical and machine learning methods in a real breast cancer problem. Artificial intelligence in medicine, 50(2),105-115.

[4]-Ben-Hur, A., Horn, D., Siegelmann, H. T., & Vapnik, V. (2001). Support vector clustering. Journal of machine learning research, 2(Dec), 125-137.

[5]-Min, F., Hu, Q., & Zhu, W. (2014). Feature selection with test cost constraint. International Journal of Approximate Reasoning, 55(1), 167-179.

[6].Otukei, J. R., & Blaschke, T. (2010). Land cover change assessment using decision trees, support vector machines and maximum likelihood classification algorithms. International Journal of Applied Earth Observation and Geoinformation, 12, §27-S31.

[7]-Kubat, M., Holte, R. C., & Matwin, S. (1998). Machine learming for the detection of oil spills in satellite radar images. Machine learning, 30(2-3), 195-215.

[8].Azam, N., & Yao, J. (2014). Analyzing uncertainties of probabilistic rough set regions with game-theoretic rough sets. International Journal of Approximate Reasoning, 55(1), 142-155.

[9].Caesarendra, W., Widodo, A., & Yang, B. S. (2010). Application of relevance vector machine and logistic regression for machine degradation assessment. Mechanical Systems and Signal Processing, 24(4), 1161-1171.

[10].Chen, D., He, Q., & Wang, X. (2010). FRSVMs: Fuzzy rough set based support vector machines. Fuzzy Sets and Systems, 161(4), 596-607.

[11]-Hosseinifard, B., Moradi, M. H., & Rostami, R. (2013). Classifying depression patients and normal subjects using machine learning techniques and nonlinear features from EEG signal. Computer methods and programs in biomedicine, 109(3), 339-345.

[12].Delen, D. (2010). A comparative analysis of machine learning techniques for student retention management. Decision Support Systems, 49(4), 498-506.

[13].Tsanas, A., & Xifara, A. (2012). Accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools. Energy and Buildings, 49, 560-567.

[14].Ahmad, S., Kalra, A., & Stephen, H. (2010). Estimating soil moisture using remote sensing data: A machine learning approach. Advances in Water Resources, 33(1), 69-80.

[15].Schuler, C. J., Christopher Burger, H., Harmeling, S., & Scholkopf, B. (2013). A machine learning approach for non-blind image deconvolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,1067-1074.

[16]. Lary, D. J., Alavi, A. H., Gandomi, A. H., & Walker, A. L. (2016). Machine learning in geosciences and remote sensing. Geoscience Frontiers, 7(1), 3-10.

[17]. Parish, E. J., & Duraisamy, K. (2016). A paradigm for data-driven predictive modelling using field inversion and machine learning. Journal of Computatio

